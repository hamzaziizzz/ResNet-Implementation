{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "004265e7",
   "metadata": {
    "id": "d714b305"
   },
   "source": [
    "### Downloading Zipped Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf155077",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "QY7BjRMJJ08k",
    "outputId": "6f35acd4-1551-421f-a895-a2c7cb79d66f"
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/df00b7jnoiok5gc/Brain-Tumor-Dataset.zip?dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08556a2",
   "metadata": {
    "id": "de04693b"
   },
   "source": [
    "### Creating Directory to Move Unzipped Data to this Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd94a14",
   "metadata": {
    "id": "kgW8gAVhKErZ"
   },
   "outputs": [],
   "source": [
    "!mkdir Brain-Tumor-Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8011804",
   "metadata": {
    "id": "b6b1f18e"
   },
   "source": [
    "### Unzipping Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc049a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "SgUEfnGEKFBT",
    "outputId": "ea610ec5-f1b0-433b-fd01-64ef3398d1d5"
   },
   "outputs": [],
   "source": [
    "!unzip Brain-Tumor-Dataset.zip?dl=0 -d Brain-Tumor-Dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d025f6b",
   "metadata": {
    "id": "7fad1ca9"
   },
   "source": [
    "# Residual Networks (ResNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f51acd",
   "metadata": {
    "id": "6aaf12b1"
   },
   "source": [
    "__ResNet__ or __Residual Neural Network__ was proposed by ([He et al.](https://arxiv.org/pdf/1512.03385.pdf)) researchers at Microsoft Research namely, _Kaiming He_, _Xiangyu Zhang_, _Shaoqing Ren_ and, _Jian Sun_; which allow us to train much deeper networks than were previously practically feasible. Also, ResNet won [ImageNet](https://www.image-net.org/about.php) Challenge in 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51cab7e",
   "metadata": {
    "id": "422a74b7"
   },
   "source": [
    "<center>\n",
    "    <img src=\"https://github.com/hamzaziizzz/ResNet-Implementation/blob/main/img/MicrosoftResearch.png?raw=1\" alt=\"Microsoft Research\" width=\"500px\"> <br>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a2445b",
   "metadata": {
    "id": "eff683ac"
   },
   "source": [
    "## Problem with Very Deep Neural Networks\n",
    "The main benefit of a very deep network is that it can represent very complex functions. It can also learn features at many different levels of abstraction, from edges (at the lower layers) to very complex features (at the deeper layers). However, using a deeper network doesn't always help. A huge barrier to training them is vanishing gradients: very deep networks often have a gradient signal that goes to zero quickly, thus making gradient descent unbearably slow. More specifically, during gradient descent, as you backprop from the final layer back to the first layer, you are multiplying by the weight matrix on each step, and thus the gradient can decrease exponentially quickly to zero (or, in rare cases, grow exponentially quickly and \"explode\" to take very large values).\n",
    "\n",
    "During training, we might therefore see the magnitude (or norm) of the gradient for the earlier layers descrease to zero very rapidly as training proceeds:\n",
    "![Vanishing Gradient](https://github.com/hamzaziizzz/ResNet-Implementation/blob/main/img/VanishingGradient.png?raw=1)\n",
    "<center>\n",
    "    <b><i>Figure 1:</i></b> <i>The speed of learning decreases very rapidly for the early layers as the network trains</i>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe24481c",
   "metadata": {
    "id": "d5f6b8ed"
   },
   "source": [
    "We are now going to solve this problem by building a Residual Network!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a1b908",
   "metadata": {
    "id": "5021b68d"
   },
   "source": [
    "## Building a Residual Network\n",
    "In ResNet architecture, a *“shortcut”* or a *“skip connection”* allows the gradient to be directly backpropagated to earlier layers:\n",
    "![Skip Connection](https://github.com/hamzaziizzz/ResNet-Implementation/blob/main/img/SkipConnection.png?raw=1)\n",
    "<center>\n",
    "    <b><i>Figure 2:</i></b> <i>A ResNet block showing a skip-connection</i>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff291303",
   "metadata": {
    "id": "4828011e"
   },
   "source": [
    "The image on the left shows the \"main path\" through the network. The image on the right adds a shortcut to the main path. By stacking these ResNet blocks on top of each other, you can form a very deep network.\n",
    "\n",
    "We also saw in lecture that having ResNet blocks with the shortcut also makes it very easy for one of the blocks to learn an identity function. This means that you can stack on additional ResNet blocks with little risk of harming training set performance. (There is also some evidence that the ease of learning an identity function--even more than skip connections helping with vanishing gradients--accounts for ResNets' remarkable performance.)\n",
    "\n",
    "Two main types of blocks are used in a ResNet, depending mainly on whether the input/output dimensions are same or different. We are going to implement both of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bce666b",
   "metadata": {
    "id": "74d27764"
   },
   "source": [
    "# ResNet-50 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7750e33b",
   "metadata": {
    "id": "e2462c6c"
   },
   "source": [
    "## Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdd20ec",
   "metadata": {
    "id": "da4a58eb"
   },
   "outputs": [],
   "source": [
    "# Import OpenCV which is a Computer Vision library and here we use it to deal with our image dataset\n",
    "import cv2\n",
    "\n",
    "# Import NumPy as our model trains on arrays which will be handled by NumPy\n",
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "\n",
    "# Import OS to interact with operating system\n",
    "import os\n",
    "\n",
    "# Import Math to use some mathematical computations\n",
    "import math\n",
    "\n",
    "# Import Shutil to move, copy and delete directories and sub-directories\n",
    "import shutil\n",
    "\n",
    "# Import glob module which finds all the pathnames matching a specified pattern according to the rules used by the Unix shell\n",
    "import glob\n",
    "\n",
    "# Import mayplotlib and seaborn to visualize the metrics of our model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import Keras\n",
    "import keras\n",
    "\n",
    "# Import image from keras.preprocessing which is used the deal with dataset images while preprocessing them\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# Import Keras ImageDataGenerator which is used for getting the input of the original data and further, it makes the transformation of this data on a random basis and gives the output resultant containing only the data that is newly transformed.\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Import scikit-learn metrics whivh will be used to display the metrics of the model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve, average_precision_score, PrecisionRecallDisplay, f1_score, roc_auc_score\n",
    "\n",
    "# Import Keras layers\n",
    "# Sequential class is used to build a model instance which will be constructed layer-by-layer (i.e., one layer is stacked over previous layer)\n",
    "# Model class is used to initialize the instance of our CNN architecture (here ResNet50)\n",
    "# load_model is uded to load the model from pre-existinf weights or models\n",
    "from keras.models import Sequential, Model, load_model\n",
    "\n",
    "# Import optimizers.Optimizer is an algorithm which is used for the learning of the model\n",
    "# SGD (Stochastic Gradient Descent) is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs.\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Import EarlyStopping and ModelCheckpoint\n",
    "# EarlyStopping is used to halt the learning of the used when the models accuracy does not improve by any significant amount\n",
    "# ModelCheckpoint is used to save the model or weights in a checkpoint file at some time interval\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Import some Keras layers features which will be used to provide input to the layer, add layer to the model, perform pooling padding and convlution, flattening the image, and batch normalization\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, MaxPool2D\n",
    "\n",
    "# Import glorot_uniform\n",
    "# GlorotUniform draws samples from a uniform distribution within [-limit, limit] , where limit = sqrt(6 / (fan_in + fan_out)) ( fan_in is the number of input units in the weight tensor and fan_out is the number of output units).\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "# Import binary_crossentropy\n",
    "# Binary cross entropy compares each of the predicted probabilities to actual class output which can be either 0 or 1. It then calculates the score that penalizes the probabilities based on the distance from the expected value. That means how close or far from the actual value.\n",
    "from keras.losses import binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5e8d28",
   "metadata": {
    "id": "ee86a5f2"
   },
   "source": [
    "## Splitting Dataset into Training, Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27de3d40",
   "metadata": {
    "id": "e7b715d7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = \"Brain-Tumor-Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb1e7c5",
   "metadata": {
    "id": "c02af1c7"
   },
   "outputs": [],
   "source": [
    "number_of_images = {}\n",
    "\n",
    "# label the number of images in each class of our dadaset\n",
    "for dir in os.listdir(DATASET_PATH):\n",
    "    # os.listdir() is used to list or count the number of images in each directory of dataset\n",
    "    # os.path.join() is used to join the parent directory, any subdirectory and the contents of the directory\n",
    "    number_of_images[dir] = len(os.listdir(os.path.join(DATASET_PATH, dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e2e30d",
   "metadata": {
    "id": "6de26b56"
   },
   "source": [
    "Randomly display any image from each class of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149920b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1005
    },
    "id": "3cd872b4",
    "outputId": "bcea0ff9-0e1b-49e8-caf0-926d8f7cfa82"
   },
   "outputs": [],
   "source": [
    "# listing all the subdirectories in our main dataset directory\n",
    "for dir in os.listdir(DATASET_PATH):\n",
    "    # picking a random image from each class of the datset\n",
    "    for img in np.random.choice(os.listdir(os.path.join(DATASET_PATH, dir)), size = 2):\n",
    "        # read each image with the help of OpenCV\n",
    "        image = cv2.imread(os.path.join(DATASET_PATH, dir, img))\n",
    "        # plot the image using matplotlib\n",
    "        plt.imshow(image); plt.axis(\"off\")\n",
    "        # give image a title using matplotlib\n",
    "        plt.title(dir)\n",
    "        # show or display the image using matplotlib\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e35794",
   "metadata": {
    "id": "10ab494d"
   },
   "source": [
    "___Listing the number of images in each class of our dataset___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a893340",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "86934816",
    "outputId": "f5d45378-c6ba-4b33-cd0c-b519c8c4f823"
   },
   "outputs": [],
   "source": [
    "print(\"The dataset contains the following:\")\n",
    "for i, each_class in enumerate(number_of_images):\n",
    "    print(f\"{i + 1}) {number_of_images[each_class]} images of {each_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005750f1",
   "metadata": {
    "id": "d2d79bc6"
   },
   "source": [
    "### Declaring and Defining our Custom Function for Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24f302",
   "metadata": {
    "id": "5c834a55"
   },
   "outputs": [],
   "source": [
    "def data_folder(folder_name, split_ratio):\n",
    "    \"\"\"\n",
    "    This function will split the dataset in a given number of new folders namely, Training, Testing and Validation in a given ratio.\n",
    "    Such that Training:Testing:Validation = x:y:z\n",
    "    Where,\n",
    "        x is the number of images in Training Folder\n",
    "        y is the number of images in Testing Folder\n",
    "        z is the number of images in Validation Folder\n",
    "    \n",
    "    Arguments:\n",
    "        folder_name: Name of the folder created for splitting the dataset\n",
    "        split_ratio: Percentage of images of original dataset for every splitted folder_name\n",
    "    \"\"\"\n",
    "    \n",
    "    # checking if the folder does not already exist\n",
    "    if not os.path.exists(\"./\" + folder_name):\n",
    "        # if the folder doesn't already exist then create that folder\n",
    "        print(f\"Creating {folder_name}\")\n",
    "        os.mkdir(\"./\" + folder_name)\n",
    "    \n",
    "    # listing all the subdirectories in our main dataset directory\n",
    "    for dir in os.listdir(DATASET_PATH):\n",
    "        # checking if that subdirectory for this folder does not already exist\n",
    "        if not os.path.exists(\"./\" + folder_name + \"/\" + dir):\n",
    "            # if it does not already exist then create that subdirectory for this folder\n",
    "            print(f\"Creating {dir} directory for {folder_name} folder\")\n",
    "            os.makedirs(\"./\" + folder_name + \"/\" + dir)\n",
    "\n",
    "        # picking random images from each class of the datset and copying it to the Training, Testing or Validation folder\n",
    "        # size for each directory is the product of number of images in each class and the ratio of train, test, validate folders\n",
    "        # For example: if Buffalo contains 1000 images and ratio train:test:validate = 70:15:15, then Testing folder will contain 700 images, Testing folder 150 images and Validation Folder 150 images \n",
    "        for img in np.random.choice(a = os.listdir(os.path.join(DATASET_PATH, dir)), size = (math.floor(split_ratio * number_of_images[dir])), replace = False):\n",
    "            \n",
    "            # pathname of original dataset\n",
    "            O = os.path.join(DATASET_PATH, dir, img)\n",
    "            \n",
    "            # pathname of splitted dataset\n",
    "            D = os.path.join(\"./\" + folder_name, dir)\n",
    "            \n",
    "            # copy each image from original dataset path to splitted dataset path\n",
    "            shutil.copy(O, D)\n",
    "    \n",
    "    else:\n",
    "        print(f\"{folder_name} folder exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abbbefb",
   "metadata": {
    "id": "fc0b00d1"
   },
   "source": [
    "___Creating Training Folder___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4282fa2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ebe09200",
    "outputId": "4ab75ee6-bd34-4412-b908-c151022ede38",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_folder(\"Training\", 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee1db70",
   "metadata": {
    "id": "9f4a37aa"
   },
   "source": [
    "___Creating Validation Folder___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874c0bff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "50ec0e22",
    "outputId": "48f08687-e89a-4b43-8a16-6e1e4a64bbf0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_folder(\"Validation\", 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5aa7de",
   "metadata": {
    "id": "35a43108"
   },
   "source": [
    "___Creating Testing Folder___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1039f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "7135e492",
    "outputId": "2102c7db-3341-4e6a-87d7-44d1b3601545",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_folder(\"Testing\", 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd17e07f",
   "metadata": {
    "id": "29ea3321"
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"./Training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0351746a",
   "metadata": {
    "id": "2a61441b"
   },
   "outputs": [],
   "source": [
    "TEST_PATH = \"./Testing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac800f06",
   "metadata": {
    "id": "15dd96d4"
   },
   "outputs": [],
   "source": [
    "VALIDATE_PATH = \"./Validation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ac477d",
   "metadata": {
    "id": "b32f74bb"
   },
   "source": [
    "___Listing the number of images in each class of our Training Folder___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f0f00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "56589a7c",
    "outputId": "99002819-31e7-4fd5-dbdf-2f6f81294e8f"
   },
   "outputs": [],
   "source": [
    "train_images = {}\n",
    "\n",
    "# label the number of images in each class of our dadaset\n",
    "for dir in os.listdir(TRAIN_PATH):\n",
    "    # os.listdir() is used to list or count the number of images in each directory of dataset\n",
    "    # os.path.join() is used to join the parent directory, any subdirectory and the contents of the directory\n",
    "    train_images[dir] = len(os.listdir(os.path.join(TRAIN_PATH, dir)))\n",
    "\n",
    "print(\"The Training folder contains the following:\")\n",
    "for i, each_class in enumerate(train_images):\n",
    "    print(f\"{i + 1}) {train_images[each_class]} images of {each_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe91382",
   "metadata": {
    "id": "fdb45467"
   },
   "source": [
    "___Listing the number of images in each class of our Validation Folder___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a07b8b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "5aaf47b8",
    "outputId": "b1edd35d-c24a-47cd-a5a0-77625b0d56bd"
   },
   "outputs": [],
   "source": [
    "validate_images = {}\n",
    "\n",
    "# label the number of images in each class of our dadaset\n",
    "for dir in os.listdir(VALIDATE_PATH):\n",
    "    # os.listdir() is used to list or count the number of images in each directory of dataset\n",
    "    # os.path.join() is used to join the parent directory, any subdirectory and the contents of the directory\n",
    "    validate_images[dir] = len(os.listdir(os.path.join(VALIDATE_PATH, dir)))\n",
    "\n",
    "print(\"The Validation folder contains the following:\")\n",
    "for i, each_class in enumerate(validate_images):\n",
    "    print(f\"{i + 1}) {validate_images[each_class]} images of {each_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a088b69",
   "metadata": {
    "id": "20d5a481"
   },
   "source": [
    "___Listing the number of images in each class of our Testing Folder___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8ac9d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "b7805edb",
    "outputId": "c1ba5a81-8922-4af9-9a31-5e58bb266380"
   },
   "outputs": [],
   "source": [
    "test_images = {}\n",
    "\n",
    "# label the number of images in each class of our dadaset\n",
    "for dir in os.listdir(TEST_PATH):\n",
    "    # os.listdir() is used to list or count the number of images in each directory of dataset\n",
    "    # os.path.join() is used to join the parent directory, any subdirectory and the contents of the directory\n",
    "    test_images[dir] = len(os.listdir(os.path.join(TEST_PATH, dir)))\n",
    "\n",
    "print(\"The Testing folder contains the following:\")\n",
    "for i, each_class in enumerate(test_images):\n",
    "    print(f\"{i + 1}) {test_images[each_class]} images of {each_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226cf60f",
   "metadata": {
    "id": "873deeec"
   },
   "source": [
    "## Preprocessing Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be0424",
   "metadata": {
    "id": "54852a52"
   },
   "source": [
    "**ImageDataGenerator()** is used to perfrom augmentation on a given image. Augmentation means that the images are duplicated with some kind of variations that increase the size of the teraining set without aquiring new images.\n",
    "Augmentation (or variations) that we performed here are the following:\n",
    "- *zoom_range:* zoom in or zoom out images in a given range\n",
    "- *width_shift_range:* shift the images horizontally in a given range\n",
    "- *height_shift_range:* shift the images vertically in a given range\n",
    "- *shear_range:* compress an vertically or horizontally in a given range, the oringinal image is somewhat distorted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09365e0e",
   "metadata": {
    "id": "9c8c76af"
   },
   "outputs": [],
   "source": [
    "train_data_generator = ImageDataGenerator(\n",
    "    zoom_range = 0.15,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.15\n",
    ")\n",
    "\n",
    "validate_data_generator = ImageDataGenerator()\n",
    "\n",
    "test_data_generator = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4a518",
   "metadata": {
    "id": "ddfa4c7d"
   },
   "source": [
    "***flow_from_directory()*** takes the path to a directory & generates batches of augmented data. <br>\n",
    "**Arguments:**\n",
    "- *directory:* string, or path to the directory.\n",
    "- *target_size:* Tuple of integers (height, width), defaults to (256,256). The dimensions to which all images found will be resized.\n",
    "- *batch_size:* Size of the batches of data (default: 32).\n",
    "- *shuffle:*  Whether to shuffle the data (default: True) If set to False, sorts the data in alphanumeric order.\n",
    "- *class_mode:* One of \"categorical\", \"binary\", \"sparse\", \"input\", or None. Default: \"categorical\". Determines the type of label arrays that are returned:\n",
    "    - \"categorical\" will be 2D one-hot encoded labels,\n",
    "    - \"binary\" will be 1D binary labels, \"sparse\" will be 1D integer labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaa5083",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "f9961d62",
    "outputId": "7cbb1bd9-487f-4f44-d009-fd5997e64bea",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_generator = train_data_generator.flow_from_directory(\n",
    "    directory = TRAIN_PATH,\n",
    "    target_size = (224, 224),\n",
    "    batch_size = 32,\n",
    "    shuffle = True,\n",
    "    class_mode = 'binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74a2bf1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ea575264",
    "outputId": "ba95b5f7-c5f2-4b7b-d18a-cc2bbb747aad"
   },
   "outputs": [],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50d0e2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "d7f2e459",
    "outputId": "a15a2d25-b83f-479d-b580-b35965474f6b"
   },
   "outputs": [],
   "source": [
    "validate_generator = validate_data_generator.flow_from_directory(\n",
    "    directory = VALIDATE_PATH,\n",
    "    target_size = (224, 224),\n",
    "    batch_size = 32,\n",
    "    shuffle = False,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41440418",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "c251d07d",
    "outputId": "82e14dce-1239-4ba7-da0b-89e1670aefb0"
   },
   "outputs": [],
   "source": [
    "test_generator = test_data_generator.flow_from_directory(\n",
    "    directory = TEST_PATH,\n",
    "    target_size = (224, 224),\n",
    "    batch_size = 32,\n",
    "    shuffle = False,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1c0d0d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "039effe9",
    "outputId": "56418203-6b90-4536-867b-da2eeea7e4ab"
   },
   "outputs": [],
   "source": [
    "test_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bb8a75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "41b8df74",
    "outputId": "ee6140ce-f248-4316-9005-5c4ff934bce5"
   },
   "outputs": [],
   "source": [
    "labels = ['Brain Tumor', 'Healthy']\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cc5075",
   "metadata": {
    "id": "6c675bd9"
   },
   "source": [
    "## Defining the Structure of ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb111820",
   "metadata": {
    "id": "a03d63b3"
   },
   "source": [
    "### Identity Block\n",
    "The identity block is the standard block used in ResNets, and corresponds to the case where the input activation (say $a^{[l]}$) has the same dimension as the output activation (say $a^{[l+2]}$). To flesh out the different steps of what happens in a ResNet's identity block, here is an alternative diagram showing the individual steps:\n",
    "![Identity Block](https://github.com/hamzaziizzz/ResNet-Implementation/blob/main/img/IdentityBlock.png?raw=1)\n",
    "<center>\n",
    "    <b><i>Figure 3:</i></b> <b>Identity Block.</b> <i>Skip connection \"skips over\" 2 layers.</i>\n",
    "</center>\n",
    "\n",
    "The upper path is the \"shortcut path.\" The lower path is the \"main path.\" In this diagram, we have also made explicit the CONV2D and ReLU steps in each layer. To speed up training we have also added a BatchNorm step. Don't worry about this being complicated to implement--you'll see that BatchNorm is just one line of code in Keras!\n",
    "\n",
    "In this exercise, you'll actually implement a slightly more powerful version of this identity block, in which the skip connection \"skips over\" 3 hidden layers rather than 2 layers. It looks like this:\n",
    "![Identity Block](https://github.com/hamzaziizzz/ResNet-Implementation/blob/main/img/IdentityBlock2.png?raw=1)\n",
    "<center>\n",
    "    <b><i>Figure 4:</i></b> <b>Identity Block.</b> <i>Skip connection \"skips over\" 3 layers.</i>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b840b2",
   "metadata": {
    "id": "2f2e4008"
   },
   "source": [
    "Here're the individual steps.\n",
    "\n",
    "First component of main path:\n",
    "- The first CONV2D has $F_1$ filters of shape (1, 1) and a stride of (1, 1). Its padding is \"valid\" and its name should be `conv_name_base + '2a'`. Use 0 as the seed for the random initialization.\n",
    "- The first BatchNorm is normalizing the channels axis. Its name should be `bn_name_base + '2a'`.\n",
    "- Then apply the ReLU activation function. This has no name and no hyperparameters.\n",
    "\n",
    "Second component of main path:\n",
    "- The second CONV2D has $F_2$ filters of shape $(f, f)$ and a stride of (1, 1). Its padding is \"same\" and its name should be `conv_name_base + '2b'`. Use 0 as the seed for the random initialization.\n",
    "- The second BatchNorm is normalizing the channels axis. Its name should be `bn_name_base + '2b'`.\n",
    "- Then apply the ReLU activation function. This has no name and no hyperparameters.\n",
    "\n",
    "Third component of main path:\n",
    "- The third CONV2D has $F_3$ filters of shape (1, 1) and a stride of (1, 1). Its padding is \"valid\" and its name should be `conv_name_base + '2c'`. Use 0 as the seed for the random initialization.\n",
    "- The third BatchNorm is normalizing the channels axis. Its name should be `bn_name_base + '2c'`. Note that there is no ReLU activation function in this component.\n",
    "\n",
    "Final step:\n",
    "- The shortcut and the input are added together.\n",
    "- Then apply the ReLU activation function. This has no name and no hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3195eb",
   "metadata": {
    "id": "c320bb79"
   },
   "outputs": [],
   "source": [
    "def identity_block(X, f, filters, stage, block):\n",
    "    \"\"\"\n",
    "    Implementation of the identity block as defined in Figure 3\n",
    "    \n",
    "    Arguments:\n",
    "        X: input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "        f: integer, specifying the shape of the middle CONV's window for the main path\n",
    "        filters: python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "        stage: integer, used to name the layers, depending on their position in the network\n",
    "        block: string/character, used to name the layers, depending on their position in the network\n",
    "    \n",
    "    Returns:\n",
    "        X: output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "   \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    # Save the input value. You'll need this later to add back to the main path.\n",
    "    X_shortcut = X\n",
    "   \n",
    "    \"\"\"First component of main path\"\"\"\n",
    "    # CONV2D\n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    # Batch Norm\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
    "    # ReLU\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    \"\"\"Second component of main path\"\"\"\n",
    "    # CONV2D\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    # Batch Norm\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
    "    # ReLU\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    \"\"\"Third component of main path\"\"\"\n",
    "    # CONV2D\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    # Batch Norm\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
    "\n",
    "    \"\"\"Final step: Add shortcut value to main path, and pass it through a RELU activation\"\"\"\n",
    "    # SKIP Connection\n",
    "    X = Add()([X, X_shortcut])\n",
    "    # ReLU\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4c45c1",
   "metadata": {
    "id": "3c9a914e"
   },
   "source": [
    "### Convolution Block\n",
    "We've implemented the ResNet identity block. Next, the ResNet \"convolutional block\" is the other type of block. We can use this type of block when the input and output dimensions don't match up. The difference with the identity block is that there is a CONV2D layer in the shortcut path:\n",
    "![Convolution Block](https://github.com/hamzaziizzz/ResNet-Implementation/blob/main/img/ConvolutionBlock.png?raw=1)\n",
    "<center>\n",
    "    <b><i>Figure 5:</i></b> <i>Convolution Block.</i>\n",
    "</center>\n",
    "\n",
    "The CONV2D layer in the shortcut path is used to resize the input $x$ to a different dimension, so that the dimensions match up in the final addition needed to add the shortcut value back to the main path. For example, to reduce the activation dimensions's height and width by a factor of 2, you can use a 1x1 convolution with a stride of 2. The CONV2D layer on the shortcut path does not use any non-linear activation function. Its main role is to just apply a (learned) linear function that reduces the dimension of the input, so that the dimensions match up for the later addition step. <br>\n",
    "The details of the convolutional block are as follows:\n",
    "\n",
    "First component of main path:\n",
    "- The first CONV2D has $F_1$ filters of shape (1, 1) and a stride of $(s, s)$. Its padding is \"valid\" and its name should be `conv_name_base + '2a'`.\n",
    "- The first BatchNorm is normalizing the channels axis. Its name should be `bn_name_base + '2a'`.\n",
    "- Then apply the ReLU activation function. This has no name and no hyperparameters.\n",
    "\n",
    "Second component of main path:\n",
    "- The second CONV2D has $F_2$ filters of $(f, f)$ and a stride of (1, 1). Its padding is \"same\" and it's name should be `conv_name_base + '2b'`.\n",
    "- The second BatchNorm is normalizing the channels axis. Its name should be `bn_name_base + '2b'`.\n",
    "- Then apply the ReLU activation function. This has no name and no hyperparameters.\n",
    "\n",
    "Third component of main path:\n",
    "- The third CONV2D has $F_3$ filters of (1, 1) and a stride of (1, 1). Its padding is \"valid\" and it's name should be `conv_name_base + '2c'`.\n",
    "- The third BatchNorm is normalizing the channels axis. Its name should be `bn_name_base + '2c'`. Note that there is no ReLU activation function in this component.\n",
    "\n",
    "Shortcut path:\n",
    "- The CONV2D has $F_3$ filters of shape (1, 1) and a stride of $(s, s)$. Its padding is \"valid\" and its name should be `conv_name_base + '1'`.\n",
    "- The BatchNorm is normalizing the channels axis. Its name should be `bn_name_base + '1'`.\n",
    "\n",
    "Final step:\n",
    "- The shortcut and the main path values are added together.\n",
    "- Then apply the ReLU activation function. This has no name and no hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd92b10",
   "metadata": {
    "id": "421a8015"
   },
   "outputs": [],
   "source": [
    "def convolutional_block(X, f, filters, stage, block, s=2):\n",
    "    \"\"\"\n",
    "    Implementation of the convolutional block as defined in Figure 4\n",
    "    \n",
    "    Arguments:\n",
    "        X: input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "        f: integer, specifying the shape of the middle CONV's window for the main path\n",
    "        filters: python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "        stage: integer, used to name the layers, depending on their position in the network\n",
    "        block: string/character, used to name the layers, depending on their position in the network\n",
    "        s: Integer, specifying the stride to be used\n",
    "    \n",
    "    Returns:\n",
    "        X: output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "   \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "    \"\"\"First component of main path\"\"\"\n",
    "    # CONV2D\n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    # Batch Norm\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
    "    # ReLU\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    \"\"\"Second component of main path\"\"\"\n",
    "    # CONV2D\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    # Batch Norm\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
    "    # ReLU\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    \"\"\"Third component of main path\"\"\"\n",
    "    # CONV2D\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    # Batch Norm\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
    "\n",
    "    \"\"\"Shortcut Path\"\"\"\n",
    "    # Shortcut CONV2D\n",
    "    X_shortcut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '1', kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
    "    # Shortcut Batch Norm\n",
    "    X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)\n",
    "\n",
    "    \"\"\"Final step: Add shortcut value to main path, and pass it through a RELU activation\"\"\"\n",
    "    # Shortcut or Skip Connection\n",
    "    X = Add()([X, X_shortcut])\n",
    "    # ReLU\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d578375c",
   "metadata": {
    "id": "bbc65402"
   },
   "source": [
    "## Building ResNet Model (50 Layers)\n",
    "The ResNet-50 model consists of 5 stages each with a convolution and Identity block. Each convolution block has 3 convolution layers and each identity block also has 3 convolution layers. The ResNet-50 has over 23 million trainable parameters. <br>\n",
    "The following figure describes in detail the architecture of this neural network. \"ID BLOCK\" in the diagram stands for \"Identity block,\" and \"ID BLOCK x3\" means we should stack 3 identity blocks together.\n",
    "![ResNet-50 Architecture](https://github.com/hamzaziizzz/ResNet-Implementation/blob/main/img/ResNet50.png?raw=1)\n",
    "<center>\n",
    "    <b><i>Figure 6:</i></b> <i>ResNet-50 Model</i>\n",
    "</center>\n",
    "\n",
    "The details of this ResNet-50 model are: <br>\n",
    "- Zero-padding pads the input with a pad of (3, 3)\n",
    "- Stage 1:\n",
    "    - The 2D Convolution has 64 filters of shape (7, 7) and uses a stride of (2, 2). Its name is *\"conv1\"*.\n",
    "    - BatchNorm is applied to the channels axis of the input.\n",
    "    - MaxPooling uses a (3, 3) window and a (2, 2) stride.\n",
    "- Stage 2:\n",
    "    - The convolutional block uses three set of filters of size [64, 64, 256], *\"f\"* is 3, *\"s\"* is 1 and the block is *\"a\"*.\n",
    "    - The 2 identity blocks use three set of filters of size [64, 64, 256], *\"f\"* is 3 and the blocks are *\"b\"* and *\"c\"*.\n",
    "- Stage 3:\n",
    "    - The convolutional block uses three set of filters of size [128, 128, 512], *\"f\"* is 3, *\"s\"* is 2 and the block is *\"a\"*.\n",
    "    - The 3 identity blocks use three set of filters of size [128, 128, 512], *\"f\"* is 3 and the blocks are *\"b\"*, *\"c\"* and *\"d\"*.\n",
    "- Stage 4:\n",
    "    - The convolutional block uses three set of filters of size [256, 256, 1024], *\"f\"* is 3, *\"s\"* is 2 and the block is *\"a\"*.\n",
    "    - The 5 identity blocks use three set of filters of size [256, 256, 1024], *\"f\"* is 3 and the blocks are *\"b\"*, *\"c\"*, *\"d\"*, *\"e\"* and *\"f\"*.\n",
    "- Stage 5:\n",
    "    - The convolutional block uses three set of filters of size [512, 512, 2048], *\"f\"* is 3, *\"s\"* is 2 and the block is *\"a\"*.\n",
    "    - The 2 identity blocks use three set of filters of size [512, 512, 2048], *\"f\"* is 3 and the blocks are *\"b\"* and *\"c\"*.\n",
    "- The 2D Average Pooling uses a window of shape (2, 2) and its name is *\"avg_pool\"*.\n",
    "- The flatten doesn't have any hyperparameters or name.\n",
    "- The Fully Connected (Dense) layer reduces its input to the number of classes using a softmax activation. Its name should be `'fc' + str(classes)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f8feea",
   "metadata": {
    "id": "58b9259e"
   },
   "outputs": [],
   "source": [
    "def ResNet50(input_shape=(224, 224, 3)):\n",
    "    \"\"\"\n",
    "    Implementation of the popular ResNet50 the following architecture:\n",
    "        CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3 -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
    "\n",
    "    Arguments:\n",
    "        input_shape: shape of the images of the dataset\n",
    "        classes: integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "        model: a Model() instance in Keras\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # ZERO PAD\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "    \n",
    "    \"\"\"Stage 1\"\"\"\n",
    "    # CONV\n",
    "    X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    # Batch Norm\n",
    "    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
    "    # ReLU\n",
    "    X = Activation('relu')(X)\n",
    "    # MAX POOL\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    \n",
    "    \"\"\"Stage 2\"\"\"\n",
    "    # CONV BLOCK\n",
    "    X = convolutional_block(X, f=3, filters=[64, 64, 256], stage=2, block='a', s=1)\n",
    "    # ID BLOCK x2\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\"Stage 3\"\"\"\n",
    "    # CONV BLOCK\n",
    "    X = convolutional_block(X, f=3, filters=[128, 128, 512], stage=3, block='a', s=2)\n",
    "    # ID BLOCK x3\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    \n",
    "    \"\"\"Stage 4\"\"\"\n",
    "    # CONV BLOCK\n",
    "    X = convolutional_block(X, f=3, filters=[256, 256, 1024], stage=4, block='a', s=2)\n",
    "    # ID BLOCK x5\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    \n",
    "    \"\"\"Stage 5\"\"\"\n",
    "    # CONV BLOCK\n",
    "    X = convolutional_block(X, f=3, filters=[512, 512, 2048], stage=5, block='a', s=2)\n",
    "    # ID BLOCK x2\n",
    "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "    # AVGPOOL. Use \"X = AveragePooling2D(...)(X)\"\n",
    "    X = AveragePooling2D(pool_size=(2, 2), padding='same')(X)\n",
    "    \n",
    "    # Create Model\n",
    "    model = Model(inputs=X_input, outputs=X, name='ResNet50')\n",
    "\n",
    "    # return ResNet50 as model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f990b6",
   "metadata": {
    "id": "01785d1f"
   },
   "source": [
    "***Defining Base Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb8b26e",
   "metadata": {
    "id": "f7ea029f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_model = ResNet50(input_shape = (224, 224, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cff0c3",
   "metadata": {
    "id": "67bf9894"
   },
   "source": [
    "***Creating Output Layer***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aa4674",
   "metadata": {
    "id": "74d22fc1"
   },
   "outputs": [],
   "source": [
    "head_model = base_model.output\n",
    "\n",
    "# Flatten the output of our model\n",
    "head_model = Flatten()(head_model)\n",
    "\n",
    "# Constructing fully connected layer\n",
    "head_model = Dense(256, activation='relu', name='fc1', kernel_initializer=glorot_uniform(seed=0))(head_model)\n",
    "head_model = Dense(128, activation='relu', name='fc2', kernel_initializer=glorot_uniform(seed=0))(head_model)\n",
    "head_model = Dense(1, activation='sigmoid', name='fc3', kernel_initializer=glorot_uniform(seed=0))(head_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a7aae",
   "metadata": {
    "id": "fd5fe178"
   },
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    inputs = base_model.input, \n",
    "    outputs = head_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee527ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "f9e86b52",
    "outputId": "306ff998-61cc-4e4d-d397-0957551a2d81"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa796d",
   "metadata": {
    "id": "0e2a0022"
   },
   "source": [
    "### `compile` method\n",
    "Configures the model for training.\n",
    "#### *Arguments*\n",
    "- **optimizer:** String (name of optimizer) or optimizer instance.\n",
    "- **loss:** Loss function. May be a string (name of loss function), or a `keras.losses` instance.\n",
    "- **metrics:** List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a keras.metrics instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb6043b",
   "metadata": {
    "id": "8c7389a3"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = keras.losses.binary_crossentropy,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f2fb9c",
   "metadata": {
    "id": "5a7d5b6f"
   },
   "source": [
    "## Defining and Initializing Callbacks for our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3ed435",
   "metadata": {
    "id": "89a77b4f"
   },
   "source": [
    "### Early Stopping\n",
    "Stop training when a monitored metric has stopped improving.\n",
    "\n",
    "Assuming the goal of a training is to maximize the validation accuracy. With this, the metric to be monitored would be 'val_accuracy', and mode would be 'max'.\n",
    "\n",
    "#### *Arguments*\n",
    "- **monitor:** Quantity to be monitored.\n",
    "- **min_delta:** Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.\n",
    "- **mode:** One of {\"auto\", \"min\", \"max\"}. In min mode, training will stop when the quantity monitored has stopped decreasing; in \"max\" mode it will stop when the quantity monitored has stopped increasing; in \"auto\" mode, the direction is automatically inferred from the name of the monitored quantity.\n",
    "- **verbose:** Verbosity mode, 0 or 1. Mode 0 is silent, and mode 1 displays messages when the callback takes an action.\n",
    "- **patience:** Number of epochs with no improvement after which training will be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa14c4a",
   "metadata": {
    "id": "e91effa7"
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor = 'val_accuracy',\n",
    "    min_delta = 0.01,\n",
    "    mode = 'max',\n",
    "    verbose = 1,\n",
    "    patience = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5717b1db",
   "metadata": {
    "id": "88cc2729"
   },
   "source": [
    "### Model Checkpoint\n",
    "ModelCheckpoint() callback is used in conjunction with training using model.fit() or model.fit_generator to save a model or weights (in a checkpoint file) at some interval, so the model or weights can be loaded later to continue the training from the state saved.\n",
    "\n",
    "A few options this callback provides include:\n",
    "- Whether to only keep the model that has achieved the \"best performance\" so far, or whether to save the model at the end of every epoch regardless of performance.\n",
    "- Definition of 'best'; which quantity to monitor and whether it should be maximized or minimized.\n",
    "- The frequency it should save at. Currently, the callback supports saving at the end of every epoch, or after a fixed number of training batches.\n",
    "- Whether only weights are saved, or the whole model is saved.\n",
    "\n",
    "#### *Arguments*\n",
    "- **filepath:** string or, path to save the model file.\n",
    "- **monitor:** The metric name to monitor.\n",
    "    - Prefix the name with \"val_\" to monitor validation metrics.\n",
    "    - Use \"loss\" or \"val_loss\" to monitor the model's total loss.\n",
    "    - If you specify metrics as strings, like \"accuracy\", pass the same string (with or without the \"val_\" prefix).\n",
    "- **save_best_only:** if save_best_only=True, it only saves when the model is considered the \"best\" and the latest best model according to the quantity monitored will not be overwritten.\n",
    "- **mode:** one of {'auto', 'min', 'max'}. If save_best_only=True, the decision to overwrite the current save file is made based on either the maximization or the minimization of the monitored quantity. For val_accuracy, this should be max, for val_loss this should be min, etc. In auto mode, the mode is set to max if the quantities monitored are 'accuracy' and are set to min for the rest of the quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f00708",
   "metadata": {
    "id": "c8ab6059"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath = './model/best_model.h5',\n",
    "    monitor = 'val_accuracy',\n",
    "    mode = 'max',\n",
    "    save_best_only = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cd54b4",
   "metadata": {
    "id": "08b8c0e9"
   },
   "source": [
    "### model.fit_generator()\n",
    "*(Deprecated) Fits the model on data yielded batch-by-batch by a generator.* <br>\n",
    "The generator is run in parallel to the model, for efficiency. For instance, this allows you to do real-time data augmentation on images on CPU in parallel to training your model on GPU. If we have a large dataset that cannot be loaded in the RAM at once, fit_generator() is the recommended way of working.\n",
    "\n",
    "#### *Arguments*\n",
    "- **generator:** A generator (e.g. like the one provided by `flow_from_directory()`).\n",
    "    The output of the generator must be a list of one of these forms:\n",
    "    ```\n",
    "    - (inputs, targets)\n",
    "    - (inputs, targets, sample_weights)\n",
    "    ```\n",
    "    This list (a single output of the generator) makes a single batch.\n",
    "- **validation_data:** this can be either:\n",
    "    - a generator for the validation data\n",
    "    - a list (inputs, targets)\n",
    "    - a list (inputs, targets, sample_weights). on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data.\n",
    "- **steps_per_epoch:** Total number of steps (batches of samples) to yield from `generator` before declaring one epoch finished and starting the next epoch. It should typically be equal to the number of samples if your dataset divided by the batch size.\n",
    "- **epochs:** Integer. Number of epochs to train the model. An epoch is an iteration over the entire data provided, as defined by `steps_per_epoch`.\n",
    "- **verbose:** Verbosity mode (0 = silent, 1 = progress bar, 2 = one line per epoch).\n",
    "- **callbacks:** List of callbacks to apply during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd17c7dc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "526345b6",
    "outputId": "6e0c43e6-e876-4998-9cf3-81767b10dbb4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_history = model.fit_generator(\n",
    "    generator = train_generator,\n",
    "    validation_data = validate_generator,\n",
    "    epochs = 100,\n",
    "    verbose = 1,\n",
    "    callbacks = [early_stopping, model_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15107bb",
   "metadata": {
    "id": "11391838"
   },
   "source": [
    "#### Plotting Accuracy vs Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ebdfaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "c8b61831",
    "outputId": "6af0d922-632f-4d9e-9e53-c6370aa72465"
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "plt.plot(model_history.history['accuracy'], label=\"Accuracy\")\n",
    "plt.plot(model_history.history['val_accuracy'], c='red', label=\"Validation Accuracy\")\n",
    "\n",
    "plt.title(\"Accuracy vs Validation Accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad97ffe5",
   "metadata": {
    "id": "88724b41"
   },
   "source": [
    "#### Plotting Loss vs Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aa5bbb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "450efa08",
    "outputId": "de09925c-85b9-4c99-d695-2c28053f84b3"
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "plt.plot(model_history.history['loss'], label=\"Loss\")\n",
    "plt.plot(model_history.history['val_loss'], c='red', label=\"Validation Loss\")\n",
    "\n",
    "plt.title(\"Loss vs Validation Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df621b",
   "metadata": {
    "id": "f12d5ccf"
   },
   "source": [
    "### model.load_weights()\n",
    "Loads all layer weights from a saved files.\n",
    "#### *Arguments*\n",
    "- **filepath:** String, path to the weights file to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796bcddd",
   "metadata": {
    "id": "1894c520"
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"./model/best_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa0995e",
   "metadata": {
    "id": "c19f267e"
   },
   "source": [
    "### model.evaluate_generator()\n",
    "*(Deprecated)* Evaluates the model on a data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee54230b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "0a7beff5",
    "outputId": "ebe31b56-4bff-4425-dcb9-8cef619c3738"
   },
   "outputs": [],
   "source": [
    "evaluation = model.evaluate_generator(test_generator)\n",
    "\n",
    "print(f\"The accuracy of our model on Testing Data is {(evaluation[1] * 100):.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa81957d",
   "metadata": {
    "id": "cb4b80e0"
   },
   "source": [
    "### model.predict_generator()\n",
    "*(Deprecated)* Generates predictions for the input samples from a data generator.\n",
    "#### *Arguments*\n",
    "- **generator:** Generator yielding batches of input samples.\n",
    "- **steps:** Total number of steps (batches of samples) to yield from `generator` before stopping.\n",
    "- **verbose:** verbosity mode, 0 or 1.\n",
    "- **workers:** Maximum number of threads to use for parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c39775",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "288f31e4",
    "outputId": "11e7146c-0acd-446b-e0c9-ea2999358720"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(\n",
    "    generator = test_generator,\n",
    "    steps = np.ceil(test_generator.samples / test_generator.batch_size),\n",
    "    verbose = 0,\n",
    "    workers = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc94d01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "69b80582",
    "outputId": "db596055-266c-4c45-f74a-3acb626c558a"
   },
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a11844",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "0ab5a5d5",
    "outputId": "1ad72b6d-2064-46ed-dd3e-b7d5cf933e97"
   },
   "outputs": [],
   "source": [
    "# number of images in our testing dataset\n",
    "test_generator.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39f647d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "747a2bbf",
    "outputId": "d93cadf1-29c6-42c5-c64d-062ad308a4d0"
   },
   "outputs": [],
   "source": [
    "# batch size for testing data generator\n",
    "test_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46f78d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "cb74e6b6",
    "outputId": "9f9405c6-057b-4e57-9c56-22d641313922"
   },
   "outputs": [],
   "source": [
    "# store the predicted outcomes of our model\n",
    "\n",
    "# if the prediction probability is greater than 0.5, then it belongs to class 0 (i.e., Zebra) otherwise class 1 (i.e., Buffalo)\n",
    "predicted_classes = np.where(predictions > 0.5, 1, 0)\n",
    "\n",
    "print(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61875cd8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "84412ae8",
    "outputId": "7e77f256-d8f5-48a9-c2ee-e7565561e715"
   },
   "outputs": [],
   "source": [
    "# classes found in our testing data generator\n",
    "print(test_generator.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9f2da5",
   "metadata": {
    "id": "4219db4c"
   },
   "source": [
    "## Metrics for our ResNet-50 Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d93e8fa",
   "metadata": {
    "id": "3b16b216"
   },
   "source": [
    "### Confustion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db4b9ae",
   "metadata": {
    "id": "3a93b705"
   },
   "outputs": [],
   "source": [
    "c_m = confusion_matrix(test_generator.classes, predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b45419",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "id": "2768e023",
    "outputId": "6c472abe-874e-4589-a780-eb3c6366ac1b"
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"ticks\")\n",
    "\n",
    "# Setting default size of the plot\n",
    "# Setting default fontsize used in the plot\n",
    "plt.rcParams['figure.figsize'] = (10.0, 9.0)\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "\n",
    "# Implementing visualization of Confusion Matrix\n",
    "display_c_m = ConfusionMatrixDisplay(c_m, display_labels=labels)\n",
    "\n",
    "\n",
    "# Plotting Confusion Matrix\n",
    "# Setting colour map to be used\n",
    "display_c_m.plot(cmap='OrRd', xticks_rotation=25)\n",
    "# Other possible options for colour map are:\n",
    "# 'autumn_r', 'Blues', 'cool', 'Greens', 'Greys', 'PuRd', 'copper_r'\n",
    "\n",
    "\n",
    "# Setting fontsize for xticks and yticks\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Confusion Matrix', fontsize=24)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "plt.savefig('confusion_matrix.png', transparent=True, dpi=500)\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f137c9d7",
   "metadata": {
    "id": "0a353546"
   },
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c1ed0f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ed4576fd",
    "outputId": "f01a7d2d-7f53-4293-d7e3-7446c2aef99b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Classification Report containes the following metrics:\\n\")\n",
    "print(classification_report(test_generator.classes, predicted_classes, target_names = labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aed3cd1",
   "metadata": {
    "id": "b4b1bfa7"
   },
   "source": [
    "### Precission-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0269840e",
   "metadata": {
    "id": "d9f5b5ad"
   },
   "outputs": [],
   "source": [
    "precision, recall, threshold = precision_recall_curve(test_generator.classes, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f840f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "id": "cf1db773",
    "outputId": "5473609b-dd02-4404-cf31-a5d567e51090"
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "PrecisionRecallDisplay(\n",
    "    precision = precision,\n",
    "    recall = recall,\n",
    "    average_precision = average_precision_score(test_generator.classes, predictions),\n",
    "    estimator_name = \"ResNet-50 Convolution Neural Network\"\n",
    ").plot(marker = \".\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8baf69a",
   "metadata": {
    "id": "21e157b3"
   },
   "source": [
    "### ROC-AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77d9a67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "id": "ba44b243",
    "outputId": "2ec81a13-f69b-42b9-f96d-23ceb39f84f3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, roc_auc_score, roc_curve, RocCurveDisplay\n",
    "\n",
    "false_positive_rate, true_positive_rate, threshold = roc_curve(test_generator.classes, predictions)\n",
    "\n",
    "RocCurveDisplay(\n",
    "    fpr = false_positive_rate,\n",
    "    tpr = true_positive_rate,\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate),\n",
    "    estimator_name = \"ResNet-50 Convolution Neural Network\"\n",
    ").plot(marker = \".\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d9905a",
   "metadata": {
    "id": "7e441dea"
   },
   "source": [
    "## Perform Prediction on Some Random Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0184cb",
   "metadata": {
    "id": "a6d64f33"
   },
   "source": [
    "### model.predict()\n",
    "Generates output predictions for the input samples.\n",
    "#### *Arguments*\n",
    "- **x:** Input samples. It could be:\n",
    "    - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs).\n",
    "    - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs).\n",
    "    - A generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6f621",
   "metadata": {
    "id": "4aaeaee8"
   },
   "source": [
    "### numpy.expand_dims() \n",
    "Expand the shape of an array. <br>\n",
    "Insert a new axis that will appear at the axis position in the expanded array shape.\n",
    "#### *Arguments*\n",
    "- **a:** *array_like* <br> Input array.\n",
    "- **axis:** *int or tuple of ints* <br> Position in the expanded axes where the new axis (or axes) is placed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f36cb37",
   "metadata": {
    "id": "974d799c"
   },
   "outputs": [],
   "source": [
    "def predict_image(image_path):\n",
    "    \"\"\"\n",
    "    This function will take image path as an argument and predict whether it is a buffalo or zebra.\n",
    "    \n",
    "    Arguments:\n",
    "        image_path: string, or path of the image where it is located.\n",
    "    \"\"\"\n",
    "    \n",
    "    # read the image from image_path with the help of OpenCV\n",
    "    image = cv2.imread(image_path)\n",
    "    # resize the image to (224, 224) as our model accepts the input of size (224, 224)\n",
    "    resized_image = cv2.resize(image, (224, 224))\n",
    "    \n",
    "    prediction = model.predict(np.expand_dims(a = resized_image, axis = 0), verbose = 0)\n",
    "    \n",
    "    if prediction < 0.5:\n",
    "        print(\"Predicted Label: Healthy\")\n",
    "    else:\n",
    "        print(\"Predicted Label: Brain Tumor\")\n",
    "    \n",
    "    plt.imshow(image); plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a4b269",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1018
    },
    "id": "5ede521f",
    "outputId": "5004a831-3d3a-4a27-e976-57eff2360d60"
   },
   "outputs": [],
   "source": [
    "# list all the directories in our main dataset path\n",
    "for directory in os.listdir(DATASET_PATH):\n",
    "    # pick a single random image from each sub-directory of main dataset path\n",
    "    for img in np.random.choice(os.listdir(os.path.join(DATASET_PATH, directory)), size = 1):\n",
    "        # perform prediction on each image that is being chosen\n",
    "        predict_image(os.path.join(DATASET_PATH, directory, img))\n",
    "        print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
